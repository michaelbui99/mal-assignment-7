{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions we have developed in earlier assignnments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outliers_df(df: pd.DataFrame, ignore_cols = []):\n",
    "    result = pd.DataFrame()\n",
    "    ignore_cols = set(ignore_cols)\n",
    "    for col in df.columns.values:\n",
    "        if col in ignore_cols:\n",
    "            continue\n",
    "        q1, q3 = np.percentile(df[col].to_numpy(), [25,75])\n",
    "        iqr = q3-q1\n",
    "        lower_boundary = q1 - (1.5*iqr)\n",
    "        upper_boundary = q3 + (1.5 * iqr)\n",
    "        upper_outliers = df.query(f'{col} > {upper_boundary}')[col]\n",
    "        lower_outliers = df.query(f'{col} < {lower_boundary}')[col]\n",
    "        result[f'{col}_outliers'] = pd.concat([upper_outliers, lower_outliers])\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def random_imputation(df_to_impute: pd.DataFrame, features: list[str]) -> pd.DataFrame:\n",
    "    copy = df_to_impute.copy()\n",
    "    for feature in features:\n",
    "        copy[feature + '_imp'] = copy[feature]\n",
    "        number_missing = copy[feature].isnull().sum()\n",
    "        observed_values = copy.loc[copy[feature].notnull(), feature]\n",
    "        copy.loc[copy[feature].isnull(), feature + '_imp'] = np.random.choice(observed_values, number_missing, replace = True)\n",
    "    return copy\n",
    "\n",
    "def bootstrapped_stochastic_imputation(df_to_impute: pd.DataFrame, features: list[str]): \n",
    "    copy = df_to_impute.copy()\n",
    "    df = random_imputation(copy, features)\n",
    "    random_data = pd.DataFrame(columns = [\"Ran\" + name for name in features])\n",
    "\n",
    "    for feature in features:\n",
    "            \n",
    "        random_data[\"Ran\" + feature] = df[feature + '_imp']\n",
    "        parameters = list(set(df.columns) - set(features) - {feature + '_imp'})\n",
    "        \n",
    "        model = LinearRegression()\n",
    "        model.fit(X = df[parameters], y = df[feature + '_imp'])\n",
    "        \n",
    "        predict = model.predict(df[parameters])\n",
    "        std_error = (predict[df[feature].notnull()] - df.loc[df[feature].notnull(), feature + '_imp']).std()\n",
    "        \n",
    "        random_predict = np.random.normal(size = df[feature].shape[0], \n",
    "                                        loc = predict, \n",
    "                                        scale = std_error)\n",
    "        random_data.loc[(df[feature].isnull()) & (random_predict > 0), \"Ran\" + feature] = random_predict[(df[feature].isnull()) & \n",
    "                                                                                (random_predict > 0)]\n",
    "    for feature in features:\n",
    "        copy[feature].fillna(random_data[f'Ran{feature}'],inplace=True)\n",
    "\n",
    "    return copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pandas.io.formats.info import DataFrameInfo\n",
    "\n",
    "def get_missing_count_ratio_df(df: pd.DataFrame):\n",
    "    row_count, col_count = df.shape\n",
    "    info = DataFrameInfo(data = df)\n",
    "    info_df= pd.DataFrame(\n",
    "        {'Non-Null Count': info.non_null_counts, 'Dtype': info.dtypes}\n",
    "    )\n",
    "\n",
    "    # Calculating missing data per column. \n",
    "    info_df['Missing Count'] = row_count- info_df['Non-Null Count']\n",
    "    info_df['Missing Ratio'] = (info_df['Missing Count'] / row_count).astype(float)\n",
    "    # Sorting missing data from highest % to lowest %\n",
    "    return info_df.sort_values(by=['Non-Null Count'], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Analysis: Frame the problem and look at the big picture.\n",
    "\n",
    "## Objective\n",
    "To create a regression model that can predict house sales prices based on different features of the house. This can help people to determine if they are selling their house at the correct price. \n",
    "\n",
    "## Framing\n",
    "This will utilize supervised learning, since we are doing regression to predict the sales price and the sales price is known for each house.\n",
    "\n",
    "## Performance Measuring\n",
    "Performance will be measured by different metrics related to regression such as coeffecient of determination ($R^2$) and Mean Squared Error (MSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is from a Kaggle competition and can be found here: https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data?select=train.csv.\n",
    "\n",
    "You must join the competition in order to get access to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_missing_count_ratio_df(eda).head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are features where almost all the data is missing such as PoolQC, MiscFeature, Alley and more. We will now look further into the features where a signifcant part of the data is missing to attempt to get an understanding of why they are missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the rows where it is not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.loc[~eda[\"PoolQC\"].isna()][\"PoolQC\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 7 rows have a value for Pool Quality, so the missing values might be because the house does not have a pool. We can verify this by looking at the PoolArea feature for the rows where PoolQC is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.loc[eda[\"PoolQC\"].isna()][[\"PoolQC\", \"PoolArea\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.loc[eda[\"PoolQC\"].isna()][[\"PoolQC\", \"PoolArea\"]].query(\"PoolArea > 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PoolQC is missing because they do not have a pool. According to `data_description.txt` this should have been set to `NA`, so we can replace the missing values with `NA` during preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will take a look at `MiscFeature`. `MiscFeature` is Miscellaneous feature not covered in other categories such as Elevator, 2nd Garage, Tennis Court and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.loc[~eda[\"MiscFeature\"].isna()][\"MiscFeature\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature has missing values because the house might not have any Miscellaneous features. We will verify this by checkign the `MiscVal` feature that indicates the value of the feature in dollars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.loc[eda[\"MiscFeature\"].isna()][[\"MiscFeature\", \"MiscVal\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.loc[eda[\"MiscFeature\"].isna()][[\"MiscFeature\", \"MiscVal\"]].query(\"MiscVal > 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would seem like `MiscFeature` has missing values because the house has no Miscellaneous features and they have not been put down as `NA` as specified in `data_description.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.loc[~eda[\"Alley\"].isna()][\"Alley\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that `Alley` also should have been classified as NA but has been put down as null instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Fence`, `MasVnrType`, `FireplaceQu` feature has missing values for the same reason as `PoolQC`, `MiscFeatures` and `Alley`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first select all numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_numerical = eda.select_dtypes([float, int])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def boxplot_columns(df: pd.DataFrame, plots: int):\n",
    "    df = df.loc[:, df.max().sort_values(ascending=False).index]\n",
    "    column_bins = np.array(list(map(lambda x: math.floor(x), np.linspace(0, len(df.columns.values), num=plots))))\n",
    "    prev = 0\n",
    "    curr = 1\n",
    "    for column_idx in column_bins[1:]:\n",
    "        # curr = column_idx if curr == 0 else column_idx - 1\n",
    "        curr = column_idx \n",
    "        plt.figure(figsize=(15,10))\n",
    "        df[df.columns.values[prev:curr]].boxplot()\n",
    "        plt.show()\n",
    "        prev = curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def plot_histograms(df: pd.DataFrame, plots: int = 5):\n",
    "    df = df.loc[:, df.max().sort_values(ascending=False).index]\n",
    "    column_bins = np.array(list(map(lambda x: math.floor(x), np.linspace(0, len(df.columns.values), num=plots))))\n",
    "    prev = 0\n",
    "    curr = 1\n",
    "    for column_idx in column_bins[1:]:\n",
    "        # curr = column_idx if curr == 0 else column_idx - 1\n",
    "        curr = column_idx\n",
    "        plt.figure(figsize=(15,10))\n",
    "        df[df.columns.values[prev:curr]].hist()\n",
    "        plt.show()\n",
    "        prev = curr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the boxplots for all numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target is `SalePrice`, so we drop that before plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_columns(eda_numerical.drop('SalePrice', axis=1), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially when looking at the boxplots, we notice that a lot of the features have outliers, including `LotArea, MiscVAl, TotalBsmtSF, BsmtFinSF1, GrLivArea, lstFlSF, BsmtUnfSF, MasVnrArea, BsmtFinSF2, GarageArea, WoodDeckSF, PoolArea, LowQualFinSF, EnclosedPorc, OpenPorchSF, 3SsnPorch, ScreenPorch, LotFrontage, MSSubClass, TotRmsAbcGrid, OverallCond, BedRoomAbvGr, GarageCars, Fireplaces, KitchenAbvGr, BsmtFullBath, BstmfHalfBath`. At this moment, we are unsure if they are natural outliers or if they represent data entry errors or processing errors. We take a closer look to see if we can learn more about the outliers. Some features are rare for common houses, meaning when a certain price class is reached the feature suddently becomes common. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of the features with outliers have IQR close to 0 and then outliers above the upper tick. This can be explained by many houses does not have the feature that the feature is describing, thus making the other houses that actually have the features to outliers. E.g. Most houses do not have an `EnclosedPorch`, but the few that do, will then be outliers. This is related to the points we made above. Some features are also related to the price "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the boxplots above, we can see that many features have outliers, but we are not sure if they are natural outliers or if they are data entry errors or processing errors. We will take a closer look at the features with outliers to see if we can learn more about them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_numerical['SalePrice'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the distribution of the SalePrice it looks positive skewed, which would seem naturally. Only a few percentage of the houses are very expensive, while most houses are in the lower to middle class.\n",
    "This can also explain some of our outliers. They are natural outliers, because the feature maybe is inherently tied to the price range of the house. We can verify this by taking a look at the correlation matrix and compare the distribution of one of the features that is higly correlated to the Sale Price. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to see plot the correlations to see "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "corr = eda_numerical.corr()\n",
    "plt.figure(figsize=(25,15))\n",
    "\n",
    "sns.heatmap(corr)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that `GrLivArea` is positively correlated with `SalePrice`, so we would assume that `GrLivArea` follow a similar distribution to the `SalePrice`, thus we can assume that the `GrLivArea` outliers are natural.  \n",
    "The same reasoning can be applied to the other features with outliers - They are either rare features of a house such as `EnclosedPorch` or they are correlated to the price class of the house such as `GrLivArea` and `OverallQual`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_numerical[['GrLivArea', 'SalePrice', 'OverallQual']].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the observations we have made, we deem the outliers to be natural and we will not remove them from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histograms(eda_numerical, plots=19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that some of the features are positive-skwewed. Of the postiive-skewed features, some of them are very positive-skewed such as `WoodDeckSF` and `OpenPorchSF`, while some of them are only slightly positive-skewed. For the very positive-skewed features we can apply a log(n) transformation and for the data that is only slightly positive-skewed we can apply a sqrt(n) transformation. \n",
    "\n",
    "When training the models, we will first train the model without the transformations and then after apply the transformations and retrain, to see if it makes an siginificant impact. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target / y in this dataset is the `SalePrice` features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleansed = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will clean missing data we identified in the exploratory data analysis. The missing data was caused by `NA` being loaded as nulls, so we will replace the missing data with `NA` or something similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_missing_count_ratio_df(eda).head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fills it nulls with NA string for the features where they can have a NA category according to data_description.txt\n",
    "def fill_missing_categorical_data(df: pd.DataFrame, features: list[str] = ['PoolQC', 'MiscFeature', 'Alley','Fence', 'MasVnrType', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageCond','GarageQual', 'BsmtExposure', 'BsmtFinType2', 'BsmtFinType1', 'BsmtCond', 'BsmtQual']) -> pd.DataFrame:\n",
    "    for feature in features:\n",
    "        df[feature] = df[feature].fillna(f'NA_{feature}')\n",
    "    return df\n",
    "\n",
    "# Fills out nulls with 0 for features where they are null because they are missing, e.g. for MasVnrArea, where the house might not have Masonry Veneer, so the area should be 0.\n",
    "def fill_missing_numerical_data(df: pd.DataFrame, features: list[str] = ['MasVnrArea', 'LotFrontage']) -> pd.DataFrame:\n",
    "    for feature in features:\n",
    "        df[feature] = df[feature].fillna(0)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleansed = fill_missing_numerical_data(fill_missing_categorical_data(data_cleansed))\n",
    "data_cleansed = data_cleansed.dropna(subset=['Electrical'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_missing_count_ratio_df(data_cleansed).head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know from the correlation matrix that GarageYrBlt is highly correlated with BltYr, so we handle the missing values by dropping the columns entirely in the feature selection section "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping `GarageYrBlt` and `Id` features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleansed = data_cleansed.drop(columns=['GarageYrBlt', 'Id'], axis=1)\n",
    "data_cleansed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a mix of categorical and numerical features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we onehot encode the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode_categorical_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.get_dummies(df, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleansed = onehot_encode_categorical_features(data_cleansed)\n",
    "data_cleansed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currenty our dataset has 302 features and we would like to reduce the amount of features before training the models. We can reduce the features by manually dropping highly correlated features using the correlation matrix, by Lasso Regression or by using PCA.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Xy(df: pd.DataFrame):\n",
    "    return df.drop('SalePrice', axis=1), df['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume Lasso Regression is better suited for the task, since PCA only reduces feature space by retaining the maximum variance, while Lasso Regression performs feature selection based by feature importance. Features that do not have high importance in regards to the target wills shrink to zero, leaving the important features back. \n",
    "\n",
    "Before we perform Lasso Regression, we must first scale the data to ensure all features are regularized correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scale the data we use a Standard Scaler, since it is less sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = get_Xy(data_cleansed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standardScaler = StandardScaler()\n",
    "\n",
    "X_scaled = standardScaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "hyperparam_grid = {'alpha': [0.01, 0.1, 1, 5, 10, 20], 'fit_intercept': [True, False]}\n",
    "lasso_gs = GridSearchCV(estimator=Lasso(), param_grid=hyperparam_grid)\n",
    "lasso_gs.fit(X_scaled,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X.columns\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_estimator = lasso_gs.best_estimator_\n",
    "lasso_estimator.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_estimator.coef_.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "selected_features_df = pd.DataFrame()\n",
    "for idx, feature in enumerate(X.columns.values):\n",
    "    selected_features_df[feature] = [lasso_estimator.coef_[idx]]\n",
    "# selected_features_df[X.columns.values] = lasso_estimator.coef_.reshape(-1, 1)\n",
    "selected_features_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_df[selected_features_df[X.columns.values] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having performed Lasso Regression, we have reduced the features from 301 to 248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"default\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Short-list promising models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first quickly try different regression models such as OLS, Random Forest, Gradient Boosted Trees, ElasticNet and K-Nearest Neighbour Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[selected_features_df.columns.values]\n",
    "X_scaled = standardScaler.fit_transform(X)\n",
    "y = y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_performance_metrics(model, X_train, y_train, X_test, y_test):\n",
    "    print(\"R^2 - train:\", model.score(X_train, y_train))\n",
    "    print(\"R^2 - test:\", model.score(X_test, y_test))\n",
    "    mse_train = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))\n",
    "    mse_test = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test))\n",
    "    print(\"MSE - train:\", mse_train)\n",
    "    print(\"MSE - test:\", mse_test)\n",
    "    print(\"MSE DIF test-train\", mse_test-mse_train)\n",
    "\n",
    "%matplotlib inline\n",
    "def plot_predictions_to_actual(model, X_train, y_train, X_test, y_test):\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.scatter(x=[x for x in range(len(X_train))], y=model.predict(X_train))\n",
    "    plt.title(\"Training set predictions\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.scatter(x=[x for x in range(len(X_test))], y=model.predict(X_test))\n",
    "    plt.title(\"Test set predictions\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=4242)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick prototyping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linearRegression = LinearRegression()\n",
    "\n",
    "linearRegression.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance_metrics(linearRegression, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions_to_actual(linearRegression, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS generalize very poorly, but performed well in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_regressor = RandomForestRegressor()\n",
    "rf_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance_metrics(rf_regressor, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does preform good, but seems a bit overfittet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor()\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance_metrics(xgb, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs a bit better than the Random Forest Regressor. Still overfitted, but the MSE on the test set is lower. It seems to be generalizing better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_net = ElasticNet()\n",
    "elastic_net.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance_metrics(elastic_net, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs nearly as good as the Gradient Boosted Tree and the Random Forest, but does not overfit. Generalizes well to our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "print_performance_metrics(knn, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
